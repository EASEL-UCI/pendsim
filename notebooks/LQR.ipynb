{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Quadratic Regulator (LQR)\n",
    "LQR is a form of optimal control that allows us to stabilize a system while minimizing the \"cost\" of doing so. A unique feature of LQR is that we can tune the weighting of variables in the cost function, which allows us to prioritize certain variables over others (e.g amount of fuel used vs performance).\n",
    "\n",
    "Our cost function `J` to be minimized is given by \n",
    "\n",
    "![LQR pic 1.PNG](https://github.com/Aaronj1n/pendsim/blob/main/docs/_static/LQR%20pic%201.PNG?raw=1)\n",
    "\n",
    "`X` and `U` represent state and control actions, respectively.\n",
    "\n",
    "We use `Q` and `R` to tune the weighting of `X` and `U`. A larger value of `Q`places a higher weighting on our state, while a larger value of `R` places a higher weighting on our control input. \n",
    "\n",
    "If we want the system to stabilize the system as fast as possible without much regard for the energy required to get there, we would choose a large value of `Q` and a small value of `R`.\n",
    "\n",
    "If we want to stabilize a system using the least amount of actuator effort and we're not concerned with how long it takes, we would choose a large value of `R` and a small value of `Q`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin our LQR controller design, let's create a simulation of our pendulum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pendsim import sim, controller, viz, utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt, t_final = 0.01, 10\n",
    "pend = sim.Pendulum(\n",
    "    2.0,  # Large mass, 2.0 kg\n",
    "    1.0,  # Small mass, 1.0 kg\n",
    "    2.0,  # length of arm, 2.0 meter\n",
    "    initial_state=np.array([0.0, 0.0, 0.0, 0.0]),\n",
    ")\n",
    "def force_func(t):\n",
    "    return 10 * np.exp( -(  ((t-2.0)/0.1)**2) )\n",
    "    #return 0\n",
    "simu = sim.Simulation(dt, t_final, force_func)\n",
    "# simu = sim.Simulation(dt, t_final, lambda t: 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our choice of `Q` and `R` is how we \"tune\" our LQR controller to fit our needs.\n",
    "\n",
    "For a 1 x n state matrix `X`, our `Q` matrix should be of form n x n.\n",
    "\n",
    "Our state matrix `X` is the 1 x 4 matrix: **[x, x_dot, theta, theta_dot]**\n",
    "\n",
    "Then our `Q` matrix is the 4 x 4 diagonal matrix: \n",
    "\n",
    "![lqr Q matrix.PNG](https://github.com/Aaronj1n/pendsim/blob/main/docs/_static/lqr%20Q%20matrix.jpg?raw=1)\n",
    "\n",
    "where `a`, `b`, `c`, and `d` are weighting factors for **x**, **x_dot**, **theta**, and **theta_dot** respectively.\n",
    "\n",
    "Since `U` is a scalar in our case, `R` is allowed to be a scalar too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with an LQR controller that places a weighting of _100000_ on theta, _1000_ on theta dot, _1_ on input force, and _zero_ weighting on x or x dot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([[0,0,0,0], [0,0,0,0],[0,0,100000,0],[0,0,0,1000]])\n",
    "R = 1\n",
    "window = 10 # used in internal calculations\n",
    "lqr_controller = controller.LQR(pend, dt, Q, R, window)\n",
    "results = simu.simulate(pend, lqr_controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "ax[0].plot(results[(\"state\", \"t\")], label=r\"$\\theta$\")\n",
    "ax[0].set_ylabel(r\"$\\theta$\")\n",
    "\n",
    "ax[1].plot(results[(\"control action\", \"control action\")], label=\"Control Action\")\n",
    "ax[1].set_ylabel(\"Control Action\")\n",
    "ax[1].set_xlabel(\"Time Step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a visualization of our LQR simulation. Note: External force is red and control force is blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu = viz.Visualizer(results, pend, dt=dt, speed=1)\n",
    "ani = visu.animate()\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tuning R\n",
    "Let's see what happens as we gradually increase `R`, meaning we increase the \"punishment\" of control inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([[0,0,0,0], [0,0,0,0],[0,0,100000,0],[0,0,0,1000]])\n",
    "R = 0\n",
    "window = 10 # used in internal calculations\n",
    "\n",
    "increase_by = 0.4\n",
    "n = 24\n",
    "conts = []\n",
    "pends = [pend] * n\n",
    "R_values = []\n",
    "for _ in range(n):\n",
    "    # increase the gain\n",
    "    conts.append(controller.LQR(pend, dt, Q, R, window))\n",
    "    R_values.append(R)\n",
    "    R += increase_by\n",
    "# simulate each controller\n",
    "all_results = simu.simulate_multiple(pends, conts)\n",
    "\n",
    "nrows, ncols = 4, 4\n",
    "fig1, ax1 = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=False, figsize=(15,12))\n",
    "axn, ax_idxs = 0, {}\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax_idxs[axn] = (i, j)\n",
    "        axn += 1\n",
    "for g, (idx, res), (axi, axj) in zip(R_values, all_results.groupby(level=0), ax_idxs.values()):\n",
    "    res.index = res.index.droplevel(0)\n",
    "    ax1[axi, axj].plot(res[('state', 't')])\n",
    "    ax1[axi, axj].set_title(f'R={g:.2f}')\n",
    "# label plots\n",
    "for i in range(nrows):\n",
    "    ax1[i, 0].set_ylabel('theta (rad)')\n",
    "for j in range(ncols):\n",
    "    ax1[-1, j].set_xlabel('time (s)')\n",
    "\n",
    "print('\\n\\n\\n\\n                                                                        \\033[1m THETA VS TIME \\033[0m')\n",
    "plt.show()\n",
    "print('\\n\\n\\n\\n                                                                        \\033[1m U VS TIME \\033[0m')\n",
    "\n",
    "#Repeat for U vs time\n",
    "\n",
    "fig2, ax2 = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=False, figsize=(15,12))\n",
    "axn, ax_idxs = 0, {}\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax_idxs[axn] = (i, j)\n",
    "        axn += 1\n",
    "for g, (idx, res), (axi, axj) in zip(R_values, all_results.groupby(level=0), ax_idxs.values()):\n",
    "    res.index = res.index.droplevel(0)\n",
    "    ax2[axi, axj].plot(res[('control action', 'control action')])\n",
    "    ax2[axi, axj].set_title(f'R={g:.2f}')\n",
    "# label plots\n",
    "for i in range(nrows):\n",
    "    ax1[i, 0].set_ylabel('Control Action')\n",
    "for j in range(ncols):\n",
    "    ax1[-1, j].set_xlabel('time (s)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase `R`, we cause the controller to be more conservative with its control inputs because those control inputs are being punished more harshly. \n",
    "\n",
    "Increasing `R` can be useful if we want a more conservative controller, e.g., we want a spaceship to maneuver using less fuel.\n",
    "\n",
    "However, increasing `R` too much can cause our controller to be unable to provide sufficient control inputs to stabilize the system. \n",
    "\n",
    "In general, a large `R` value will cause our system to react \"slower.\"\n",
    "\n",
    "# Tuning Q\n",
    "Keeping `R` at _5_, let's see what happens when we vary `Q`, which increases the punishment of our state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([[0,0,0,0], [0,0,0,0],[0,0,0,0],[0,0,0,0]])\n",
    "R = 5\n",
    "window = 10 # used in internal calculations\n",
    "\n",
    "increase_c = 7e4\n",
    "increase_d = 3e4\n",
    "n = 16\n",
    "conts = []\n",
    "pends = [pend] * n\n",
    "Q_values = []\n",
    "for _ in range(n):\n",
    "    # increase the gain\n",
    "    Q_values.append(Q)\n",
    "    Q = Q + np.array([[0,0,0,0], [0,0,0,0],[0,0,increase_c,0],[0,0,0,increase_d]])\n",
    "    conts.append(controller.LQR(pend, dt, Q, R, window))\n",
    "# simulate each controller\n",
    "all_results = simu.simulate_multiple(pends, conts)\n",
    "\n",
    "nrows, ncols = 4, 4\n",
    "fig1, ax1 = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=False, figsize=(15,12))\n",
    "axn, ax_idxs = 0, {}\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax_idxs[axn] = (i, j)\n",
    "        axn += 1\n",
    "for g, (idx, res), (axi, axj) in zip(Q_values, all_results.groupby(level=0), ax_idxs.values()):\n",
    "    res.index = res.index.droplevel(0)\n",
    "    ax1[axi, axj].plot(res[('state', 't')])\n",
    "    ax1[axi, axj].set_title(f'C={g[2,2]:.2f} and D={g[3,3]:.2f}')\n",
    "# label plots\n",
    "for i in range(nrows):\n",
    "    ax1[i, 0].set_ylabel('theta (rad)')\n",
    "for j in range(ncols):\n",
    "    ax1[-1, j].set_xlabel('time (s)')\n",
    "\n",
    "print('\\n\\n\\n\\n                                                                        \\033[1m THETA VS TIME \\033[0m')\n",
    "plt.show()\n",
    "print('\\n\\n\\n\\n                                                                        \\033[1m U VS TIME \\033[0m')\n",
    "\n",
    "\n",
    "#Repeat for U vs time\n",
    "\n",
    "fig2, ax2 = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=False, figsize=(15,12))\n",
    "axn, ax_idxs = 0, {}\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax_idxs[axn] = (i, j)\n",
    "        axn += 1\n",
    "for g, (idx, res), (axi, axj) in zip(Q_values, all_results.groupby(level=0), ax_idxs.values()):\n",
    "    res.index = res.index.droplevel(0)\n",
    "    ax2[axi, axj].plot(res[('control action', 'control action')])\n",
    "    ax2[axi, axj].set_title(f'C={g[2,2]:.2f} and D={g[3,3]:.2f}')\n",
    "# label plots\n",
    "for i in range(nrows):\n",
    "    ax2[i, 0].set_ylabel('Control Action')\n",
    "for j in range(ncols):\n",
    "    ax2[-1, j].set_xlabel('time (s)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase `Q`, our system stabilizes itself faster. This is because `Q` punishes deviation from the state vector [0, 0, 0, 0], that is, the neutral position. \n",
    "\n",
    "Using a very large `Q` can be useful if we want our spaceship to make a manuever as quickly as possible without as much regard for fuel consumption. \n",
    "\n",
    "Using a `Q` that's too small may cause the controller to be unresponsive, which will lead to instability. Using a `Q` that's too large may cause the controller to over-exert itself and use up too much energy/fuel. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
